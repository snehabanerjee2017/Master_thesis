{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from utils.parse import parse_args, load_model, get_clusters, get_pca, get_tsne, get_medoid_indices, calc_dbcv,get_hier_clusters\n",
    "from dataset.dataloader import data_loader\n",
    "from DBCV.DBCV_multiproc import DBCV\n",
    "# from DBCV.DBCV_neighbor import DBCV\n",
    "# from DBCV.DBCV import DBCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully load Abc with 49981 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test dataset: 100%|██████████| 391/391 [00:19<00:00, 20.43it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = yaml.load(open(\"config/config_cal_dbcv.yaml\"),Loader=yaml.FullLoader)\n",
    "test_data_loader = data_loader(config)\n",
    "with torch.cuda.device(config['util']['gpu']):\n",
    "    ema_net = load_model(config)\n",
    "    all_points = []\n",
    "    start = time.time()\n",
    "    for batch_id, test_data in tqdm(enumerate(test_data_loader, 0), total=len(test_data_loader), smoothing=0.9,desc = 'test dataset'):\n",
    "        x1_test = test_data[0]\n",
    "        x1_test = x1_test.cuda()\n",
    "        x1_test = x1_test.transpose(2, 1)\n",
    "        emb =  ema_net(x1_test,return_embedding=config['training']['return_embedding'])\n",
    "        all_points.append(emb[0].detach().cpu().numpy())\n",
    "    del ema_net\n",
    "    del test_data_loader\n",
    "    all_points = np.concatenate(all_points,axis=0)\n",
    "    end = time.time()\n",
    "    np.random.seed(42)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of dataset (1000, 1024) and it takes 22.516611099243164 seconds or 0.3752768516540527 minutes or 0.006254614194234212 hours\n",
      "Dimesnion after PCA (1000, 512) and it takes 1.5419604778289795 seconds or 0.02569934129714966 minutes or 0.0004283223549524943 hours\n",
      "Dimesnion after TSNE (1000, 2) and it takes 2.966329574584961 seconds or 0.049438826243082684 minutes or 0.0008239804373847113 hours\n"
     ]
    }
   ],
   "source": [
    "all_points = np.take(all_points, np.random.choice(np.array(list(range(0,all_points.shape[0]))), config['dbcv']['num_points'], replace=False), axis=0, out=None, mode='raise')\n",
    "    \n",
    "print(f'Dimension of dataset {all_points.shape} and it takes {end-start} seconds or {(end-start)/60} minutes or {(end-start)/3600} hours')\n",
    "\n",
    "all_points = get_pca(n_components=config['pca']['n_components'],data=all_points)\n",
    "\n",
    "# all_points = get_pca(n_components=config['pca_2']['n_components'],data=all_points)\n",
    "\n",
    "all_points = get_tsne(n_components=config['tsne']['n_components'], data=all_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hier_clusters(clf, all_points:np.ndarray, original_clusters:np.ndarray, clusters:list, cluster_members:list, rel_points: np.ndarray, min_samples:int = 2, classifier = 'hdbscan',count:int=0):\n",
    "\n",
    "    if count!=0:\n",
    "        clf, pred, num_clusters = get_clusters(data = rel_points,store_centers = 'medoid', classifier=classifier,min_samples=min_samples) \n",
    "    all_medoids = clf.medoids_\n",
    "\n",
    "    print(all_medoids.shape)\n",
    "     \n",
    "    if count!=0:\n",
    "        cluster_pointer = num_clusters\n",
    "        # No new outliers from label 2\n",
    "        for i, label in enumerate(pred):\n",
    "            if label==-1:\n",
    "                for j, cluster in enumerate(clusters[1:]):\n",
    "                    members = cluster_members[j]\n",
    "                    if np.any(members==rel_points[i]):\n",
    "                        for member in members:\n",
    "                            idx =  np.where(all_points==member)[0][0] \n",
    "                            original_clusters[idx] = cluster_pointer\n",
    "                        cluster_pointer+=1\n",
    "                        break\n",
    "\n",
    "        # reassign all members of the original clusters\n",
    "        for j, point in enumerate(rel_points):\n",
    "            if pred[j]!=-1:\n",
    "                for i, cluster in enumerate(clusters[1:]):\n",
    "                    members = cluster_members[i]\n",
    "                    if np.any(members==point):                            \n",
    "                        for member in members:\n",
    "                            idx =  np.where(all_points==member)[0][0] \n",
    "                            original_clusters[idx] = pred[j]\n",
    "                        break\n",
    "        clusters = np.unique(original_clusters).tolist()\n",
    "        cluster_members = []\n",
    "        for cluster in clusters:\n",
    "            idx = (original_clusters == cluster).nonzero()[0]\n",
    "            cluster_points = np.take(all_points,idx,axis=0)\n",
    "            cluster_members.append(cluster_points)\n",
    "        print(f'Now number of clusters are {len(np.unique(original_clusters).tolist())}') \n",
    "        print(f'The number are outliers in level {count} are {original_clusters.tolist().count(-1)}')\n",
    "\n",
    "    all_medoid_indices = get_medoid_indices(medoids=all_medoids,data=rel_points)\n",
    "\n",
    "    rel_points = np.take(rel_points,all_medoid_indices,axis=0)\n",
    "    print(f'Dimension of embedding of all representative  objects {rel_points.shape}')\n",
    "\n",
    "    if count==0:\n",
    "        num_clusters = len(np.unique(original_clusters).tolist())\n",
    "    return rel_points, original_clusters, clusters, cluster_members, num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdbscan Clustering\n",
      "The value of min_cluster_size is 2\n",
      "Number of outliers 119\n",
      "Number of clusters excluding outliers 300\n",
      "Clustering validation dataset took 0.11532783508300781 seconds\n",
      "(300, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 21559.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of embedding of all representative  objects (300, 2)\n",
      "hdbscan Clustering\n",
      "The value of min_cluster_size is 2\n",
      "Number of outliers 51\n",
      "Number of clusters excluding outliers 71\n",
      "Clustering validation dataset took 0.05086374282836914 seconds\n",
      "(71, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now number of clusters are 123\n",
      "The number are outliers in level 1 are 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:00<00:00, 23185.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of embedding of all representative  objects (71, 2)\n",
      "hdbscan Clustering\n",
      "The value of min_cluster_size is 2\n",
      "Number of outliers 7\n",
      "Number of clusters excluding outliers 18\n",
      "Clustering validation dataset took 0.03547978401184082 seconds\n",
      "(18, 2)\n",
      "Now number of clusters are 77\n",
      "The number are outliers in level 2 are 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 27393.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of embedding of all representative  objects (18, 2)\n",
      "hdbscan Clustering\n",
      "The value of min_cluster_size is 2\n",
      "Number of outliers 0\n",
      "Number of clusters excluding outliers 4\n",
      "Clustering validation dataset took 0.025417089462280273 seconds\n",
      "(4, 2)\n",
      "Now number of clusters are 63\n",
      "The number are outliers in level 3 are 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 25458.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of embedding of all representative  objects (4, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rel_points = all_points.copy()\n",
    "\n",
    "clf, pred, num_clusters = get_clusters(data = rel_points,store_centers = 'medoid', classifier=config['results']['classifier'],min_samples=config['results']['min_samples']) \n",
    "\n",
    "original_clusters = pred.copy()\n",
    "cluster_members = []\n",
    "clusters = np.unique(original_clusters).tolist()\n",
    "for cluster in clusters:\n",
    "    idx = (original_clusters == cluster).nonzero()[0]\n",
    "    cluster_points = np.take(all_points,idx,axis=0)\n",
    "    cluster_members.append(cluster_points)\n",
    "\n",
    "count=0\n",
    "\n",
    "while True:\n",
    "    rel_points, original_clusters, clusters, cluster_members, num_clusters = get_hier_clusters(clf = clf, all_points=all_points,rel_points=rel_points,  original_clusters=original_clusters, clusters = clusters, cluster_members=cluster_members, min_samples=config['results']['min_samples'], classifier='hdbscan',count=count)\n",
    "    count+=1\n",
    "\n",
    "    if num_clusters<=10:\n",
    "        n_cluster=num_clusters\n",
    "        pred = original_clusters.copy()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
