{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "torch.cuda.is_available()\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils.parse import parse_args, load_model, get_pca, get_tsne, get_clusters\n",
    "from dataset.dataloader import data_loader\n",
    "import gc\n",
    "import yaml   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from multiprocessing import Pool,cpu_count\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(\"config/config_rep_objs.yaml\"),Loader=yaml.FullLoader)\n",
    "config['training']['train'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'path': '/mnt/data/das-sb/dataset_hdf5_processed',\n",
       "  'type': 'abc',\n",
       "  'num_point': 2048,\n",
       "  'train_val_percent': 0.85,\n",
       "  'train_test_split': 0.95},\n",
       " 'model': {'path': '/home/das-sb/GIT/source_library/log/PointNet/2024-02-22_12-03/sinkhorn/PointNet/checkpoints/self_best_model.pth',\n",
       "  'name': 'BYOL',\n",
       "  'proj_dim': 256,\n",
       "  'tau': 0.01,\n",
       "  'K': 64,\n",
       "  'angle': 0.0,\n",
       "  'md': 'pn',\n",
       "  'crop': [0.85, 0.85],\n",
       "  'dims': 1024,\n",
       "  'neighs': 20},\n",
       " 'util': {'gpu': 0, 'workers': 8},\n",
       " 'training': {'train': False,\n",
       "  'unsup': True,\n",
       "  'l_type': 'gl',\n",
       "  'is_recon': True,\n",
       "  'normal': False,\n",
       "  'log_dir': 'PointNet',\n",
       "  'batch_size': 32,\n",
       "  'epoch': 200,\n",
       "  'learning_rate': 0.001,\n",
       "  'lr_decay': 0.5,\n",
       "  'lr_patience': 5,\n",
       "  'optimizer': 'AdamW',\n",
       "  'decay_rate': 0.0001,\n",
       "  'aug': 'jitter',\n",
       "  'early_stopping': True,\n",
       "  'early_stopping_patience': 10,\n",
       "  'cb': False,\n",
       "  'return_embedding': True},\n",
       " 'pca': {'n_components': 512},\n",
       " 'pca_2': {'n_components': 154},\n",
       " 'results': {'dir_path': '/mnt/data/das-sb/results'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully load Abc with 49981 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train dataset: 100%|██████████| 1562/1562 [00:41<00:00, 37.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of dataset (49981, 2048, 3)\n",
      "Dimension of dataset embedding (49981, 1024)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data_loader = data_loader(config)\n",
    "with torch.cuda.device(config['util']['gpu']):\n",
    "    ema_net = load_model(config)\n",
    "    emb_all_points = []\n",
    "    all_points = []\n",
    "    for batch_id, test_data in tqdm(enumerate(test_data_loader, 0), total=len(test_data_loader), smoothing=0.9,desc = 'train dataset'):\n",
    "        x1 = test_data[0]\n",
    "        x1 = x1.cuda()\n",
    "        x1 = x1.transpose(2, 1)\n",
    "        emb =  ema_net(x1,return_embedding=config['training']['return_embedding'])\n",
    "        emb_all_points.append(emb[0].detach().cpu().numpy())\n",
    "        all_points.append(test_data[0].detach().cpu().numpy())\n",
    "    del test_data_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() \n",
    "del ema_net\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() \n",
    "all_points = np.concatenate(all_points,axis=0)\n",
    "emb_all_points = np.concatenate(emb_all_points,axis=0)\n",
    "print(f'Dimension of dataset {all_points.shape}')\n",
    "print(f'Dimension of dataset embedding {emb_all_points.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimesnion after PCA (49981, 512) and it takes 65.20254111289978 seconds or 1.0867090185483297 minutes or 0.018111816975805495 hours\n",
      "Dimesnion after PCA (49981, 154) and it takes 20.80364203453064 seconds or 0.34672736724217734 minutes or 0.005778789454036289 hours\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49981, 154)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_all_points = get_pca(n_components=config['pca']['n_components'],data=emb_all_points)\n",
    "\n",
    "emb_all_points = get_pca(n_components=config['pca_2']['n_components'], data=emb_all_points)\n",
    "emb_all_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AgglomerativeClustering(n_clusters = 175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = clf.fit_predict(emb_all_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = np.unique(pred_val).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_medoid(cluster_points):\n",
    "    \"\"\"\n",
    "    Function to calculate the medoid of a cluster.\n",
    "    \"\"\"\n",
    "    num_points = len(cluster_points)\n",
    "    distances = np.zeros((num_points, num_points))\n",
    "    \n",
    "    # Calculate pairwise distances between points\n",
    "    for i in range(num_points):\n",
    "        for j in range(i+1, num_points):\n",
    "            distances[i][j] = distances[j][i] = directed_hausdorff(cluster_points[i], cluster_points[j])[0]\n",
    "    \n",
    "    # Calculate total distance for each point\n",
    "    total_distances = np.sum(distances, axis=0)\n",
    "    \n",
    "    # Find index of point with minimum total distance\n",
    "    medoid_index = np.argmin(total_distances)\n",
    "    \n",
    "    return cluster_points[medoid_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n",
       " [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n",
       " [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43],\n",
       " [44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54],\n",
       " [55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65],\n",
       " [66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76],\n",
       " [77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87],\n",
       " [88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98],\n",
       " [99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
       " [110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120],\n",
       " [121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131],\n",
       " [132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142],\n",
       " [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153],\n",
       " [154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164],\n",
       " [165, 166, 167, 168, 169, 170, 171, 172, 173, 174]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processes = cpu_count()\n",
    "chunk_size = len(clusters)//processes + 1\n",
    "chunks = [clusters[i:i+chunk_size] for i in range(0,len(clusters),chunk_size)]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_medoid(clusters,all_points,pred_val):\n",
    "    medoid_dict = {}\n",
    "    for cluster in clusters:\n",
    "        medoid_dict[cluster] = None\n",
    "    for cluster in clusters:\n",
    "        idx = (pred_val == cluster).nonzero()[0]\n",
    "        cluster_points = np.take(all_points,idx,axis=0)\n",
    "        medoid_dict[cluster] = calculate_medoid(cluster_points)\n",
    "    return medoid_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = partial(calc_medoid,all_points = all_points,pred_val=pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.35608198109532935, 1930, 1564)\n",
      "<class 'tuple'>\n",
      "(0.343623500972956, 1972, 1724)\n",
      "<class 'tuple'>\n",
      "(0.3803689307808596, 644, 1822)\n",
      "<class 'tuple'>\n",
      "(0.09958460394190234, 173, 1364)\n",
      "<class 'tuple'>\n",
      "(0.19961621416776737, 293, 264)\n",
      "<class 'tuple'>\n",
      "(0.43905721885878596, 1106, 1517)\n",
      "<class 'tuple'>\n",
      "(0.42005457899640297, 1943, 1100)\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "with Pool(processes) as p:\n",
    "    res = p.map(g,chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
